{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k4_mtHuxsUri"
      },
      "source": [
        "# Training a wake word detection pipeline from scratch\n",
        "\n",
        "Wake word detection is a technology used in voice recognition systems, such as virtual assistants and smart speakers, to trigger the system to start listening when a specific word or phrase is spoken.\n",
        "\n",
        "In this tutorial, you will learn how to train and evaluate a wake word pipeline from scratch."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Work with predefined wake words\n",
        "In this configuration, audios containing **wake words have been previously collected**.\n",
        "\n",
        "In particular, we will a dataset containing two set of audios:\n",
        "* positive samples represented by audio samples of the word/phrases: \"alexa\"\n",
        "* negative samples consisting of **audio where the wakeword/phrase is not present**"
      ],
      "metadata": {
        "id": "LT0teQHor_vd"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CjaF1oSPsUrk"
      },
      "source": [
        "### Data preparation\n",
        "1. We download a repository containing audio samples of the wakeword\n",
        "2. We download a sample from the [AMI corpus](https://groups.inf.ed.ac.uk/ami/corpus/) that will be used as *source* of negative sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "17YlcSzR9GgJ"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/Picovoice/wake-word-benchmark.git &> /dev/null\n",
        "!wget https://groups.inf.ed.ac.uk/ami/AMICorpusMirror/amicorpus/TS3003a/audio/TS3003a.Mix-Headset.wav &> /dev/null"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import random\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "import lightning.pytorch as pl\n",
        "\n",
        "import torchaudio as ta\n",
        "import torchaudio.transforms as T\n",
        "\n",
        "from torchvision.models import resnet18\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from IPython.display import Audio"
      ],
      "metadata": {
        "id": "Rv3PlG0_G8Mb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_waveform(waveform, sr, title=\"Waveform\", ax=None):\n",
        "    waveform = waveform.numpy()\n",
        "\n",
        "    num_channels, num_frames = waveform.shape\n",
        "    time_axis = torch.arange(0, num_frames) / sr\n",
        "\n",
        "    if ax is None:\n",
        "        _, ax = plt.subplots(num_channels, 1)\n",
        "    ax.plot(time_axis, waveform[0], linewidth=1)\n",
        "    ax.grid(True)\n",
        "    ax.set_xlim([0, time_axis[-1]])\n",
        "    ax.set_title(title)\n",
        "\n",
        "def plot_mfcc(fbank, title=None):\n",
        "    fig, axs = plt.subplots(1, 1)\n",
        "    axs.set_title(title or \"MFCC\")\n",
        "    axs.imshow(fbank, aspect=\"auto\")\n",
        "    axs.set_ylabel(\"mfcc bin\")\n",
        "    axs.set_xlabel(\"time frame\")"
      ],
      "metadata": {
        "id": "eLfu3vVKXKAh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pytorch dataset handling\n",
        "1.   We define our dataset for loading and processing 3-seconds audio samples\n",
        "2.   We implement the training dataloader\n",
        "\n"
      ],
      "metadata": {
        "id": "o9xVG_9THn1j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def is_valid_file(filename, extensions=[\".wav\", \".flac\"]):\n",
        "  \"\"\"Checks if a file is an allowed extension.\n",
        "\n",
        "  Args:\n",
        "      filename (string): path to a file\n",
        "      extensions (tuple of strings): extensions to consider (lowercase)\n",
        "\n",
        "  Returns:\n",
        "      bool: True if the filename ends with one of given extensions\n",
        "  \"\"\"\n",
        "  return filename.lower().endswith(extensions if isinstance(extensions, str) \\\n",
        "                                   else tuple(extensions))\n",
        "\n",
        "def list_directory(target_dir):\n",
        "  instances = []\n",
        "  for root, _, fnames in sorted(os.walk(target_dir, followlinks=True)):\n",
        "      for fname in sorted(fnames):\n",
        "        path = os.path.join(root, fname)\n",
        "        if is_valid_file(path):\n",
        "          instances.append(path)\n",
        "  return instances\n",
        "\n",
        "\n",
        "class WakeWordDataset(Dataset):\n",
        "  def __init__(self, pos_path, neg_path, max_length=3, sr=16000, evalmode=False):\n",
        "    pos = [[f, 1] for f in list_directory(pos_path)]\n",
        "    neg = [[neg_path, 0]] * len(pos)\n",
        "    self.data = pos + neg\n",
        "    random.shuffle(self.data)\n",
        "\n",
        "    self.max_length = max_length * sr\n",
        "    self.sr = sr\n",
        "    self.evalmode = evalmode\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.data)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    fn, label = self.data[idx]\n",
        "    audio, sr = ta.load(fn)\n",
        "    audio = ta.functional.resample(audio, sr, self.sr)\n",
        "\n",
        "    #Â Handle too short audio samples\n",
        "    audiosize = audio.size(1)\n",
        "    if audiosize <= self.max_length:\n",
        "      shortage  = self.max_length - audiosize + 1\n",
        "      audio = F.pad(audio, (0, shortage), mode='replicate')\n",
        "      audiosize = audio.size(1)\n",
        "\n",
        "    if self.evalmode:\n",
        "      startframe = torch.linspace(0, audiosize - self.max_length, 5, dtype=int)\n",
        "    else:\n",
        "      startframe = torch.randint(audiosize - self.max_length, (1,))\n",
        "\n",
        "    audios = []\n",
        "    if self.evalmode and self.max_length == 0:\n",
        "        audios.append(audio)\n",
        "    else:\n",
        "        for asf in startframe:\n",
        "            audios.append(audio[:, asf:(asf+self.max_length)])\n",
        "\n",
        "    audios = torch.stack(audios)\n",
        "    return audios, label"
      ],
      "metadata": {
        "id": "Hxp7BoLbyFfw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = WakeWordDataset(\"./wake-word-benchmark/audio/alexa\",\n",
        "                          \"./TS3003a.Mix-Headset.wav\")\n",
        "audios_pos, label = dataset[0]\n",
        "audios_neg, label = dataset[500]"
      ],
      "metadata": {
        "id": "mzjM6W7rxQBj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's listen to the audio of a positive sample from the dataset..."
      ],
      "metadata": {
        "id": "jEGKdTsLHI4A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Audio(audios_pos[0].numpy(), rate=16000)"
      ],
      "metadata": {
        "id": "3UL1p5caHIIs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "... and the audio of a negative sample from the dataset"
      ],
      "metadata": {
        "id": "SbzhX7viHX1K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Audio(audios_neg[0].numpy(), rate=16000)"
      ],
      "metadata": {
        "id": "bqGUjxop7AHZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataloader = DataLoader(dataset, shuffle=False, batch_size=64,\n",
        "                        num_workers=2, pin_memory=True)"
      ],
      "metadata": {
        "id": "STUM28UaH_Pw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "daYanXWqsUro"
      },
      "source": [
        "## Model definition\n",
        "\n",
        "We design a wake word detector using a Convolutional Neural Network (CNN) that takes audio samples represented by Mel-frequency cepstral coefficients (MFCC).\n",
        "\n",
        "Specifically, we adapt the original ResNet18 pretrained on ImageNet to encode a 40-dimensional MFCC and output a probability distribution over the two classes wake_word/generic_content.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's visualize the MFCC representation for a 3-second audio sample"
      ],
      "metadata": {
        "id": "TTvS04WZWoTf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mfcc = T.MFCC(sample_rate=16000, n_mfcc=40, melkwargs={\"n_fft\": 400,\n",
        "                                                       \"hop_length\": 160,\n",
        "                                                       \"n_mels\": 40})\n",
        "mfcc_sample = mfcc(audios_pos[0])\n",
        "plot_waveform(audios_pos[0], 16000)\n",
        "plot_mfcc(mfcc_sample[0])\n",
        "print(\"Shape of the MFCC representation \", mfcc_sample.shape)"
      ],
      "metadata": {
        "id": "A-9rAXP8Wl3A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's take a look at the ResNet18 architecture for understanding which parts might be changed"
      ],
      "metadata": {
        "id": "w84vIuaMYVOU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "resnet18()"
      ],
      "metadata": {
        "id": "NyFlXSw-Ymf9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now is the right time to define our model for wake word detection!"
      ],
      "metadata": {
        "id": "fIA7u0PyYp-Q"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iMT-1fJMsUro"
      },
      "outputs": [],
      "source": [
        "class WakeWordModel(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(WakeWordModel, self).__init__()\n",
        "\n",
        "    self.mfcc = T.MFCC(sample_rate=16000, n_mfcc=40,\n",
        "                       melkwargs={\"n_fft\": 400,\n",
        "                                  \"hop_length\": 160,\n",
        "                                  \"n_mels\": 40})\n",
        "    self.instancenorm = nn.InstanceNorm1d(40)\n",
        "    self.resnet = resnet18()\n",
        "    self.resnet.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(1, 2),\n",
        "                                  padding=3, bias=False)\n",
        "    self.resnet.maxpool = nn.Identity()\n",
        "    self.resnet.avgpool = nn.AdaptiveAvgPool2d((None, 1))\n",
        "    self.resnet.fc = nn.Linear(5*512, 2)\n",
        "\n",
        "  def forward(self, x):\n",
        "    with torch.no_grad():\n",
        "      x = self.mfcc(x) + 1e-6\n",
        "      x = self.instancenorm(x).unsqueeze(1).detach()\n",
        "    x = self.resnet(x)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = WakeWordModel()\n",
        "model(audios_pos[0])"
      ],
      "metadata": {
        "id": "jxPrZKHMNbyd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "trii9yj1sUro"
      },
      "source": [
        "## Training\n",
        "Now that the data and the model are ready, let's train with `pytorch-ligthning`!"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cpu'\n",
        "model.to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "for e in range(2): # we run the training for two epochs\n",
        "    # switch to train mode\n",
        "    model.train()\n",
        "\n",
        "    for i, (audio, target) in enumerate(dataloader):\n",
        "        # move data to the same device as model\n",
        "        audio = audio.to(device, non_blocking=True).view(audio.size(0), -1)\n",
        "        target = target.to(device, non_blocking=True)\n",
        "\n",
        "        # compute output\n",
        "        output = model(audio)\n",
        "        loss = F.cross_entropy(output, target)\n",
        "        acc = (output.argmax(1) == target).sum() / target.size(0)\n",
        "\n",
        "        # measure accuracy and record loss\n",
        "        print(\"Epoch [%d/2], iter [%d/%d], loss %.4f, acc %.2f\" % (e, i,\n",
        "                                                                   len(dataloader),\n",
        "                                                                   loss.item(),\n",
        "                                                                   acc.item()\n",
        "                                                                   )\n",
        "        )\n",
        "\n",
        "        # compute gradient and do SGD step\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()"
      ],
      "metadata": {
        "id": "BUdGKlpoag2N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HQOznETesUro"
      },
      "source": [
        "## Inference\n",
        "\n",
        "Once trained, the model can be applied to an audio sample..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G1Cckon8sUrp"
      },
      "outputs": [],
      "source": [
        "model.eval()\n",
        "\n",
        "sample, label = dataset[101]\n",
        "model(sample[0].to(device)).argmax()\n",
        "print(model(sample[0].to(device)).argmax(), label)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "...or on our own recording"
      ],
      "metadata": {
        "id": "eQXyCLQeuqLd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# all imports\n",
        "from io import BytesIO\n",
        "from base64 import b64decode\n",
        "from google.colab import output\n",
        "from IPython.display import Javascript\n",
        "\n",
        "RECORD = \"\"\"\n",
        "const sleep  = time => new Promise(resolve => setTimeout(resolve, time))\n",
        "const b2text = blob => new Promise(resolve => {\n",
        "  const reader = new FileReader()\n",
        "  reader.onloadend = e => resolve(e.srcElement.result)\n",
        "  reader.readAsDataURL(blob)\n",
        "})\n",
        "var record = time => new Promise(async resolve => {\n",
        "  stream = await navigator.mediaDevices.getUserMedia({ audio: true })\n",
        "  recorder = new MediaRecorder(stream)\n",
        "  chunks = []\n",
        "  recorder.ondataavailable = e => chunks.push(e.data)\n",
        "  recorder.start()\n",
        "  await sleep(time)\n",
        "  recorder.onstop = async ()=>{\n",
        "    blob = new Blob(chunks)\n",
        "    text = await b2text(blob)\n",
        "    resolve(text)\n",
        "  }\n",
        "  recorder.stop()\n",
        "})\n",
        "\"\"\"\n",
        "\n",
        "def record(sec=3):\n",
        "  print(\"\")\n",
        "  print(\"Speak Now...\")\n",
        "  display(Javascript(RECORD))\n",
        "  sec += 1\n",
        "  s = output.eval_js('record(%d)' % (sec*1000))\n",
        "  print(\"Done Recording !\")\n",
        "  b = b64decode(s.split(',')[1])\n",
        "  return b #byte stream"
      ],
      "metadata": {
        "id": "hSbGe4Ifuk0_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "audio = record(3)"
      ],
      "metadata": {
        "id": "ExbtcHwNu9_3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Audio(audio)"
      ],
      "metadata": {
        "id": "NLgt5lpCvB3m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "def preprocess_audio(audio, max_length=3*16000):\n",
        "  #Â Handle too short audio samples\n",
        "  audiosize = audio.size(1)\n",
        "  if audiosize <= max_length:\n",
        "    shortage  = max_length - audiosize + 1\n",
        "    audio = F.pad(audio, (0, shortage), mode='replicate')\n",
        "    audiosize = audio.size(1)\n",
        "\n",
        "  startframe = torch.linspace(0, audiosize - max_length, 5, dtype=int)\n",
        "\n",
        "  audios = []\n",
        "  for asf in startframe:\n",
        "    audios.append(audio[:, asf:(asf+max_length)])\n",
        "\n",
        "  audios = torch.cat(audios)\n",
        "  return audios\n",
        "\n",
        "np_array = np.frombuffer(audio, dtype=np.int8)\n",
        "pt_audio = torch.from_numpy(np_array).float()\n",
        "pt_audio = preprocess_audio(pt_audio[None])\n",
        "model(pt_audio)"
      ],
      "metadata": {
        "id": "uOQPN6SO1qnl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C0gyncQUsUrp"
      },
      "source": [
        "## Improve previous model\n",
        "*   Add data augmentation to input samples: noise from the RIR dataset, reverberation, and so on.\n",
        "*   Tune training parameters\n",
        "*   Improve model architecture"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## How to train a model on a custom wake word?"
      ],
      "metadata": {
        "id": "2oagCs6IzEEJ"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}