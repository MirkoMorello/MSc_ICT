{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 [Train]:   0%|                        | 4/1552 [01:46<11:24:01, 26.51s/it, loss=0.7020, acc=45.3125]"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchaudio.transforms import MFCC\n",
    "import torch.nn.functional as F\n",
    "import torchaudio\n",
    "import os\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "\n",
    "class MaskedConv1d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1,\n",
    "                 padding=0, dilation=1, groups=1, bias=True):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv1d(in_channels, out_channels, kernel_size, stride=stride,\n",
    "                              padding=padding, dilation=dilation, groups=groups, bias=bias)\n",
    "        self.padding = padding\n",
    "        self.dilation = dilation\n",
    "        self.stride = stride\n",
    "        self.kernel_size = kernel_size\n",
    "\n",
    "    def forward(self, x, length):\n",
    "        max_length = x.size(2)\n",
    "        length = torch.div(((length + 2 * self.padding - self.dilation * (self.kernel_size - 1) - 1).float() + self.stride), self.stride, rounding_mode='floor').long()\n",
    "        mask = torch.arange(max_length, device=x.device)[None, :] < length[:, None]\n",
    "        x = x * mask.unsqueeze(1)\n",
    "        x = self.conv(x)\n",
    "        return x, length\n",
    "\n",
    "class JasperBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, dropout=0.0, residual=False):\n",
    "        super().__init__()\n",
    "        self.mconv = nn.ModuleList([\n",
    "            MaskedConv1d(in_channels, in_channels, kernel_size, stride=stride, padding=padding, dilation=dilation, groups=in_channels, bias=False),\n",
    "            MaskedConv1d(in_channels, out_channels, kernel_size=1, stride=1, bias=False),\n",
    "            nn.BatchNorm1d(out_channels)\n",
    "        ])\n",
    "        self.res = None\n",
    "        if residual:\n",
    "            self.res = nn.ModuleList([\n",
    "                nn.ModuleList([\n",
    "                    MaskedConv1d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
    "                    nn.BatchNorm1d(out_channels)\n",
    "                ])\n",
    "            ])\n",
    "        self.mout = nn.Sequential(\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, length):\n",
    "        residual = x\n",
    "        res_length = length\n",
    "        out = x\n",
    "        out_length = length\n",
    "        for layer in self.mconv:\n",
    "            if isinstance(layer, MaskedConv1d):\n",
    "                out, out_length = layer(out, out_length)\n",
    "            else:\n",
    "                out = layer(out)\n",
    "        if self.res:\n",
    "          for res_layer_list in self.res:\n",
    "              res = residual\n",
    "              for layer in res_layer_list:\n",
    "                  if isinstance(layer, MaskedConv1d):\n",
    "                      res, _ = layer(res, res_length)\n",
    "                  else:\n",
    "                      res = layer(res)\n",
    "              residual = res\n",
    "        if self.res is not None:\n",
    "            out = out + residual\n",
    "        out = self.mout(out)\n",
    "        return out, out_length\n",
    "\n",
    "\n",
    "class ConvASREncoder(nn.Module):\n",
    "    def __init__(self, in_channels, blocks_params):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        for params in blocks_params:\n",
    "            layers.append(JasperBlock(**params))\n",
    "        self.encoder = nn.Sequential(*layers)\n",
    "        self.in_channels = in_channels\n",
    "\n",
    "    def forward(self, x, length):\n",
    "        for layer in self.encoder:\n",
    "            x, length = layer(x, length)\n",
    "        return x, length\n",
    "\n",
    "class ConvASRDecoderClassification(nn.Module):\n",
    "    def __init__(self, in_features, num_classes):\n",
    "        super().__init__()\n",
    "        self.pooling = nn.AdaptiveAvgPool1d(output_size=1)\n",
    "        self.decoder_layers = nn.Sequential(\n",
    "            nn.Linear(in_features, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pooling(x)\n",
    "        x = x.squeeze(2)\n",
    "        x = self.decoder_layers(x)\n",
    "        return x\n",
    "\n",
    "class AudioToMFCCPreprocessor(nn.Module):\n",
    "    def __init__(self, sample_rate=16000, n_mels=64, n_mfcc=64, n_fft=512, hop_length=160, f_min=0, f_max=8000):\n",
    "        super().__init__()\n",
    "        self.featurizer = MFCC(\n",
    "            sample_rate=sample_rate,\n",
    "            n_mfcc=n_mfcc,\n",
    "            melkwargs={\n",
    "                \"n_fft\": n_fft,\n",
    "                \"n_mels\": n_mels,\n",
    "                \"hop_length\": hop_length,\n",
    "                \"f_min\": f_min,\n",
    "                \"f_max\": f_max,\n",
    "            },\n",
    "        )\n",
    "\n",
    "    def forward(self, x, length):\n",
    "      with torch.no_grad():\n",
    "        x = self.featurizer(x)\n",
    "        return x, length\n",
    "\n",
    "\n",
    "class SpecCutout(nn.Module):\n",
    "    def __init__(self, rect_masks=5, rect_time_masks=10):\n",
    "        super().__init__()\n",
    "        self.rect_masks = rect_masks\n",
    "        self.rect_time_masks = rect_time_masks\n",
    "\n",
    "    def forward(self, specgram, length):\n",
    "        batch_size, _, time_len = specgram.shape\n",
    "        for _ in range(self.rect_time_masks):\n",
    "            cutout_length = torch.randint(0, self.rect_masks, (batch_size,))\n",
    "            offset = torch.randint(0, time_len, (batch_size,))\n",
    "            for i in range(batch_size):\n",
    "              actual_end = offset[i] + cutout_length[i]\n",
    "              if actual_end < length[i] :\n",
    "                specgram[i, :, offset[i]:actual_end] = 0\n",
    "        return specgram, length\n",
    "\n",
    "\n",
    "class SpecAugment(nn.Module):\n",
    "    def __init__(self, freq_masks=2, freq_width=27, time_masks=10, time_width=0.05):\n",
    "        super().__init__()\n",
    "        self.freq_masks = freq_masks\n",
    "        self.freq_width = freq_width\n",
    "        self.time_masks = time_masks\n",
    "        self.time_width = time_width\n",
    "\n",
    "    def forward(self, specgram, length):\n",
    "      batch_size, n_mels, time_len = specgram.shape\n",
    "      for _ in range(self.freq_masks):\n",
    "        for i in range(batch_size):\n",
    "          f = torch.randint(low=0, high=self.freq_width, size=(1,)).item()\n",
    "          f0 = torch.randint(low=0, high=n_mels - f, size=(1,)).item()\n",
    "          specgram[i, f0 : f0 + f, :] = 0\n",
    "      for _ in range(self.time_masks):\n",
    "        for i in range(batch_size):\n",
    "          t = int(self.time_width * length[i].item())\n",
    "          t0 = torch.randint(low=0, high=length[i].item() - t, size=(1,)).item()\n",
    "          specgram[i, :, t0 : t0 + t] = 0\n",
    "      return specgram, length\n",
    "\n",
    "\n",
    "\n",
    "class SpectrogramAugmentation(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.spec_cutout = SpecCutout()\n",
    "        self.spec_augment = SpecAugment()\n",
    "\n",
    "    def forward(self, x, length):\n",
    "      x, length = self.spec_cutout(x, length)\n",
    "      x, length = self.spec_augment(x, length)\n",
    "      return x, length\n",
    "\n",
    "class CropOrPadSpectrogramAugmentation(nn.Module):\n",
    "    def __init__(self, audio_length=1024):\n",
    "        super().__init__()\n",
    "        self.audio_length = audio_length\n",
    "\n",
    "    def forward(self, x, length):\n",
    "        batch_size, _, seq_len = x.size()\n",
    "        if seq_len > self.audio_length:\n",
    "            offset = torch.randint(0, seq_len - self.audio_length + 1, (batch_size,))\n",
    "            new_x = torch.zeros((batch_size, x.size(1), self.audio_length), device=x.device)\n",
    "            for i in range(batch_size):\n",
    "                new_x[i] = x[i, :, offset[i]:offset[i] + self.audio_length]\n",
    "            length = torch.tensor([self.audio_length] * batch_size, device=x.device)\n",
    "            return new_x, length\n",
    "        elif seq_len < self.audio_length:\n",
    "            pad_amount = self.audio_length - seq_len\n",
    "            padded_x = F.pad(x, (0,pad_amount) , \"constant\", 0)\n",
    "            length = length + pad_amount # do not increase length because mfcc features will be computed on padded signal\n",
    "            return padded_x, length\n",
    "        else:\n",
    "          return x, length\n",
    "\n",
    "class EncDecClassificationModel(nn.Module):\n",
    "    def __init__(self, num_classes, sample_rate=16000, n_mels=64, n_mfcc=64, n_fft=512, hop_length=160, f_min=0, f_max=8000, audio_length=16000*2):\n",
    "        super().__init__()\n",
    "        self.spec_augmentation = SpectrogramAugmentation()\n",
    "        self.crop_or_pad = CropOrPadSpectrogramAugmentation(audio_length=audio_length)\n",
    "        self.preprocessor = AudioToMFCCPreprocessor(sample_rate=sample_rate, n_mels=n_mels, n_mfcc=n_mfcc, n_fft=n_fft, hop_length=hop_length, f_min=f_min,f_max=f_max)\n",
    "        blocks_params = [\n",
    "            {\"in_channels\": n_mfcc, \"out_channels\": 128, \"kernel_size\": 11, \"stride\": 1, \"padding\": 5, \"dilation\": 1, \"dropout\": 0.0, \"residual\":False},\n",
    "            {\"in_channels\": 128, \"out_channels\": 64, \"kernel_size\": 13, \"stride\": 1, \"padding\": 6, \"dilation\": 1, \"dropout\": 0.0, \"residual\": True},\n",
    "            {\"in_channels\": 64, \"out_channels\": 64, \"kernel_size\": 15, \"stride\": 1, \"padding\": 7, \"dilation\": 1, \"dropout\": 0.0, \"residual\": True},\n",
    "            {\"in_channels\": 64, \"out_channels\": 64, \"kernel_size\": 17, \"stride\": 1, \"padding\": 8, \"dilation\": 1, \"dropout\": 0.0, \"residual\": True},\n",
    "            {\"in_channels\": 64, \"out_channels\": 128, \"kernel_size\": 29, \"stride\": 1, \"padding\": 28, \"dilation\": 2, \"dropout\": 0.0, \"residual\": False},\n",
    "            {\"in_channels\": 128, \"out_channels\": 128, \"kernel_size\": 1, \"stride\": 1, \"padding\": 0, \"dilation\": 1, \"dropout\": 0.0, \"residual\":False},\n",
    "          ]\n",
    "        self.encoder = ConvASREncoder(in_channels=n_mfcc, blocks_params=blocks_params)\n",
    "        self.decoder = ConvASRDecoderClassification(in_features=128, num_classes=num_classes)\n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "        self._accuracy = TopKClassificationAccuracy()\n",
    "\n",
    "    def forward(self, x, length, y=None):\n",
    "        x, length = self.preprocessor(x, length)\n",
    "        x, length = self.spec_augmentation(x, length)\n",
    "        x, length = self.crop_or_pad(x, length)\n",
    "        x, length = self.encoder(x, length)\n",
    "        logits = self.decoder(x)\n",
    "        if y is not None:\n",
    "          loss = self.loss(logits, y)\n",
    "          acc = self._accuracy(logits, y)\n",
    "          return loss, acc, logits\n",
    "        else:\n",
    "          return logits\n",
    "\n",
    "    def predict(self, x, length):\n",
    "      with torch.no_grad():\n",
    "        logits = self.forward(x,length)\n",
    "        return torch.argmax(logits, dim=-1)\n",
    "\n",
    "class TopKClassificationAccuracy(nn.Module):\n",
    "    def __init__(self, k=(1,)):\n",
    "        super().__init__()\n",
    "        self.k = k\n",
    "\n",
    "    def forward(self, logits, targets):\n",
    "        with torch.no_grad():\n",
    "            maxk = max(self.k)\n",
    "            batch_size = targets.size(0)\n",
    "            _, pred = logits.topk(maxk, 1, True, True)\n",
    "            pred = pred.t()\n",
    "            correct = pred.eq(targets.view(1, -1).expand_as(pred))\n",
    "            res = []\n",
    "            for k in self.k:\n",
    "                correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n",
    "                res.append(correct_k.mul_(100.0 / batch_size))\n",
    "            return res[0] if len(res) == 1 else res\n",
    "\n",
    "class SpeechCommandsDataset(Dataset):\n",
    "    def __init__(self, root_dir, keywords, split, transform=None, fixed_length=32000):\n",
    "        super().__init__()\n",
    "        self.root_dir = root_dir\n",
    "        self.keywords = keywords\n",
    "        self.split = split\n",
    "        self.transform = transform\n",
    "        self.fixed_length = fixed_length\n",
    "        self.file_paths = []\n",
    "        self.labels = []\n",
    "        self.label_to_idx = {keyword: i for i, keyword in enumerate(keywords)}\n",
    "        self.label_to_idx[\"unknown\"] = len(self.label_to_idx)\n",
    "        self.idx_to_label = {i: keyword for keyword, i in self.label_to_idx.items()}\n",
    "\n",
    "        if self.split in [\"validation\", \"testing\"]:\n",
    "            list_file = os.path.join(self.root_dir, f\"{self.split}_list.txt\")\n",
    "            with open(list_file, 'r') as f:\n",
    "                file_list = [line.strip() for line in f]\n",
    "            for file_path in file_list:\n",
    "                label = file_path.split('/')[0]\n",
    "                if label in self.keywords:\n",
    "                    self.file_paths.append(os.path.join(self.root_dir, file_path))\n",
    "                    self.labels.append(self.label_to_idx[label])\n",
    "                else:\n",
    "                    self.file_paths.append(os.path.join(self.root_dir, file_path))\n",
    "                    self.labels.append(self.label_to_idx[\"unknown\"])\n",
    "\n",
    "        elif self.split == \"training\":\n",
    "            self._create_training_data()\n",
    "\n",
    "        else:\n",
    "            raise ValueError(\"Invalid split. Must be 'training', 'validation', or 'testing'.\")\n",
    "\n",
    "        if not self.file_paths:\n",
    "            raise ValueError(f\"No .wav files found for split '{self.split}' in {root_dir}.\")\n",
    "\n",
    "        if self.split == \"training\":\n",
    "            self.balance_dataset()  # Call balance_dataset *after* creating the initial file list\n",
    "\n",
    "\n",
    "    def _create_training_data(self):\n",
    "        \"\"\"Creates the training data: keywords and unknown, excluding val/test.\"\"\"\n",
    "\n",
    "        keyword_files = []\n",
    "        unknown_files = []\n",
    "\n",
    "        all_subdirs = [d for d in os.listdir(self.root_dir) if os.path.isdir(os.path.join(self.root_dir, d))]\n",
    "\n",
    "        for subdir in all_subdirs:\n",
    "            subdir_path = os.path.join(self.root_dir, subdir)\n",
    "            files = [os.path.join(self.root_dir, subdir, f) for f in os.listdir(subdir_path) if f.endswith(\".wav\")] # Full path\n",
    "\n",
    "            if subdir in self.keywords:\n",
    "                keyword_files.extend([(f, self.label_to_idx[subdir]) for f in files])\n",
    "            elif subdir != '_background_noise_':\n",
    "                unknown_files.extend([(f, self.label_to_idx[\"unknown\"]) for f in files])\n",
    "\n",
    "        # Load validation and testing lists (full paths)\n",
    "        validation_files = set()\n",
    "        testing_files = set()\n",
    "        with open(os.path.join(self.root_dir, \"validation_list.txt\"), 'r') as f:\n",
    "            validation_files.update(os.path.join(self.root_dir, line.strip()) for line in f)  # Full path\n",
    "        with open(os.path.join(self.root_dir, \"testing_list.txt\"), 'r') as f:\n",
    "            testing_files.update(os.path.join(self.root_dir, line.strip()) for line in f)  # Full path\n",
    "\n",
    "        # Filter, using full paths for correct exclusion\n",
    "        training_keyword_files = [(f, lbl) for f, lbl in keyword_files if f not in validation_files and f not in testing_files]\n",
    "        training_unknown_files = [(f, lbl) for f, lbl in unknown_files if f not in validation_files and f not in testing_files]\n",
    "\n",
    "        # Combine, and populate self.file_paths and self.labels\n",
    "        training_files = training_keyword_files + training_unknown_files\n",
    "        for file_path, label in training_files:\n",
    "            self.file_paths.append(file_path)\n",
    "            self.labels.append(label)\n",
    "\n",
    "\n",
    "\n",
    "    def balance_dataset(self):\n",
    "        \"\"\"Balances the dataset by oversampling the minority class.\"\"\"\n",
    "\n",
    "        # Count occurrences of each label\n",
    "        label_counts = {}\n",
    "        for label in self.labels:\n",
    "            label_counts[label] = label_counts.get(label, 0) + 1\n",
    "\n",
    "        # Find the maximum count (majority class)\n",
    "        max_count = max(label_counts.values())\n",
    "\n",
    "        # Oversample the minority class\n",
    "        new_file_paths = []\n",
    "        new_labels = []\n",
    "\n",
    "        for label, count in label_counts.items():\n",
    "            indices = [i for i, l in enumerate(self.labels) if l == label]\n",
    "\n",
    "            # Add all original samples\n",
    "            for idx in indices:\n",
    "                new_file_paths.append(self.file_paths[idx])\n",
    "                new_labels.append(self.labels[idx])\n",
    "\n",
    "            # Oversample if needed\n",
    "            if count < max_count:\n",
    "                num_samples_to_add = max_count - count\n",
    "                samples_to_add = random.choices(indices, k=num_samples_to_add)  # with replacement\n",
    "                for idx in samples_to_add:\n",
    "                    new_file_paths.append(self.file_paths[idx])  # Duplicate the file path\n",
    "                    new_labels.append(self.labels[idx]) # Duplicate the label\n",
    "\n",
    "        # Update the dataset's file paths and labels\n",
    "        self.file_paths = new_file_paths\n",
    "        self.labels = new_labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_path = self.file_paths[idx]\n",
    "        label = self.labels[idx]\n",
    "        try:\n",
    "            waveform, sample_rate = torchaudio.load(file_path)\n",
    "        except RuntimeError as e:\n",
    "            print(f\"Error loading {file_path}: {e}\")\n",
    "            waveform = torch.zeros(1, self.fixed_length)\n",
    "            label = -1  # Keep -1 for failed loads\n",
    "\n",
    "        waveform = self.pad_or_trim(waveform, sample_rate)\n",
    "        length = torch.tensor([waveform.shape[1]])\n",
    "        if self.transform:\n",
    "            waveform, length = self.transform(waveform, length)\n",
    "        return waveform.squeeze(0), length.squeeze(0), label\n",
    "\n",
    "    def pad_or_trim(self, waveform, sample_rate):\n",
    "        num_frames = waveform.shape[1]\n",
    "        target_frames = self.fixed_length\n",
    "        if num_frames > target_frames:\n",
    "            waveform = waveform[:, :target_frames]\n",
    "        elif num_frames < target_frames:\n",
    "            padding = target_frames - num_frames\n",
    "            waveform = F.pad(waveform, (0, padding))\n",
    "        return waveform\n",
    "\n",
    "def create_data_loaders(root_dir, keywords, batch_size, sample_rate=16000, audio_length=32000):\n",
    "    train_dataset = SpeechCommandsDataset(root_dir, keywords, split=\"training\", fixed_length=audio_length)\n",
    "    val_dataset = SpeechCommandsDataset(root_dir, keywords, split=\"validation\", fixed_length=audio_length)\n",
    "    # test_dataset = SpeechCommandsDataset(root_dir, keywords, split=\"testing\", fixed_length=audio_length)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "    # test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "    return train_loader, val_loader  # , test_loader\n",
    "\n",
    "def train_epoch(model, train_loader, optimizer, device, epoch):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    running_accuracy = 0.0\n",
    "    total_samples = 0\n",
    "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch} [Train]\", leave=False)\n",
    "    for waveforms, lengths, labels in progress_bar:\n",
    "        valid_indices = labels != -1\n",
    "        if not torch.any(valid_indices):\n",
    "          continue\n",
    "\n",
    "        waveforms = waveforms[valid_indices].to(device)\n",
    "        lengths = lengths[valid_indices].to(device)\n",
    "        labels = labels[valid_indices].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        loss, acc, _ = model(waveforms, lengths, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * waveforms.size(0)\n",
    "        running_accuracy += acc[0].item() * waveforms.size(0)\n",
    "        total_samples += waveforms.size(0)\n",
    "        progress_bar.set_postfix({\"loss\": f\"{loss.item():.4f}\", \"acc\": f\"{acc[0].item():.4f}\"})\n",
    "\n",
    "\n",
    "    epoch_loss = running_loss / total_samples\n",
    "    epoch_accuracy = running_accuracy / total_samples\n",
    "    return epoch_loss, epoch_accuracy\n",
    "\n",
    "\n",
    "def validate_epoch(model, val_loader, device, epoch):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    running_accuracy = 0.0\n",
    "    total_samples = 0\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "    with torch.no_grad():\n",
    "        progress_bar = tqdm(val_loader, desc=f\"Epoch {epoch} [Val]\", leave=False)\n",
    "        for waveforms, lengths, labels in progress_bar:\n",
    "            valid_indices = labels != -1\n",
    "            if not torch.any(valid_indices):\n",
    "                continue\n",
    "            waveforms = waveforms[valid_indices].to(device)\n",
    "            lengths = lengths[valid_indices].to(device)\n",
    "            labels = labels[valid_indices].to(device)\n",
    "            loss, acc, logits = model(waveforms, lengths, labels)\n",
    "            running_loss += loss.item() * waveforms.size(0)\n",
    "            running_accuracy += acc[0].item() * waveforms.size(0)\n",
    "            total_samples += waveforms.size(0)\n",
    "            progress_bar.set_postfix({\"loss\": f\"{loss.item():.4f}\", \"acc\": f\"{acc[0].item():.4f}\"})\n",
    "            _, predicted = torch.max(logits, 1)\n",
    "            all_predictions.extend(predicted.cpu().tolist())\n",
    "            all_targets.extend(labels.cpu().tolist())\n",
    "    epoch_loss = running_loss / total_samples\n",
    "    epoch_accuracy = running_accuracy / total_samples\n",
    "    return epoch_loss, epoch_accuracy, all_predictions, all_targets\n",
    "\n",
    "\n",
    "# --- Configuration ---\n",
    "num_epochs = 5\n",
    "batch_size = 64\n",
    "learning_rate = 0.001\n",
    "num_classes = 2\n",
    "keywords = [\"marvin\"]\n",
    "sample_rate = 16000\n",
    "audio_length = sample_rate * 2\n",
    "\n",
    "# Dataset Path\n",
    "root_dir = \"/mnt/c/Users/Andrea/Downloads/wake_word_detection/data\"\n",
    "if not os.path.exists(root_dir):\n",
    "    raise ValueError(\"Please set root_dir to where the SpeechCommands dataset is located.\")\n",
    "\n",
    "# Device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# --- Model, Optimizer, DataLoaders ---\n",
    "model = EncDecClassificationModel(num_classes=num_classes, sample_rate=sample_rate, audio_length=audio_length).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "train_loader, val_loader = create_data_loaders(root_dir, keywords, batch_size, sample_rate, audio_length)\n",
    "\n",
    "# --- Training Loop ---\n",
    "best_val_accuracy = 0.0\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    train_loss, train_accuracy = train_epoch(model, train_loader, optimizer, device, epoch)\n",
    "    val_loss, val_accuracy, _, _ = validate_epoch(model, val_loader, device, epoch)\n",
    "\n",
    "    print(f\"Epoch {epoch}: Train Loss: {train_loss:.4f}, Train Acc: {train_accuracy:.4f}, \"\n",
    "            f\"Val Loss: {val_loss:.4f}, Val Acc: {val_accuracy:.4f}\")\n",
    "\n",
    "    if val_accuracy > best_val_accuracy:\n",
    "        best_val_accuracy = val_accuracy\n",
    "        torch.save(model.state_dict(), \"best_model.pth\")\n",
    "        print(\"Saved best model!\")\n",
    "\n",
    "print(\"Finished Training\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
