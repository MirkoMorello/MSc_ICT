{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# <center>Dataset creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchaudio\n",
    "import torchaudio.transforms as T\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "\n",
    "#################################\n",
    "# Dataset and Data Preparation  #\n",
    "#################################\n",
    "\n",
    "class SubsetSC(torchaudio.datasets.SPEECHCOMMANDS):\n",
    "    def __init__(self, subset: str = None, root=\"./datasets/speechcommand\", download=True):\n",
    "        super().__init__(root=root, download=download)\n",
    "        def load_list(filename):\n",
    "            filepath = os.path.join(self._path, filename)\n",
    "            with open(filepath) as f:\n",
    "                return [os.path.join(self._path, line.strip()) for line in f]\n",
    "        if subset == \"validation\":\n",
    "            self._walker = load_list(\"validation_list.txt\")\n",
    "        elif subset == \"testing\":\n",
    "            self._walker = load_list(\"testing_list.txt\")\n",
    "        elif subset == \"training\":\n",
    "            excludes = load_list(\"validation_list.txt\") + load_list(\"testing_list.txt\")\n",
    "            excludes = set(excludes)\n",
    "            self._walker = [w for w in self._walker if w not in excludes]\n",
    "\n",
    "\n",
    "class InMemorySpeechCommands(Dataset):\n",
    "    def __init__(self, subset=\"training\", fixed_length=16000):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          subset: One of \"training\", \"validation\", \"testing\".\n",
    "          fixed_length: The desired number of audio samples per clip.\n",
    "        \"\"\"\n",
    "        self.fixed_length = fixed_length\n",
    "        self.dataset = SubsetSC(subset=subset, root=\"./datasets/speechcommand\", download=True)\n",
    "        self.data = []\n",
    "        self.labels = []\n",
    "        \n",
    "        # Build label index from the dataset's actual folder location\n",
    "        dataset_root = self.dataset._path  # use the path used by the dataset\n",
    "        all_labels = [\n",
    "            d for d in os.listdir(dataset_root)\n",
    "            if os.path.isdir(os.path.join(dataset_root, d)) and d != '_background_noise_'\n",
    "        ]\n",
    "        self.label_set = sorted(all_labels)\n",
    "        self.label_to_idx = {label: idx for idx, label in enumerate(self.label_set)}\n",
    "        print(f\"Found {len(self.label_set)} labels: {self.label_set}\")\n",
    "        \n",
    "        # Load all samples into memory (resample, pad/trim)\n",
    "        for waveform, sample_rate, label, *_ in tqdm(self.dataset, desc=f\"Loading {subset} data\", leave=False):\n",
    "            if sample_rate != 16000:\n",
    "                resampler = T.Resample(orig_freq=sample_rate, new_freq=16000)\n",
    "                waveform = resampler(waveform)\n",
    "            waveform = waveform.squeeze(0)  # Remove channel dimension if exists\n",
    "            if waveform.size(0) > fixed_length:\n",
    "                waveform = waveform[:fixed_length]\n",
    "            elif waveform.size(0) < fixed_length:\n",
    "                waveform = F.pad(waveform, (0, fixed_length - waveform.size(0)))\n",
    "            self.data.append(waveform)\n",
    "            try:\n",
    "                self.labels.append(self.label_to_idx[label])\n",
    "            except KeyError:\n",
    "                # If a label is encountered that's not in our mapping, add it\n",
    "                # (or alternatively, you could skip it or raise an error)\n",
    "                new_idx = len(self.label_to_idx)\n",
    "                print(f\"New label found: {label}. Assigning new index {new_idx}\")\n",
    "                self.label_to_idx[label] = new_idx\n",
    "                self.label_set.append(label)\n",
    "                self.labels.append(new_idx)\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.fixed_length, self.labels[idx]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# <center>Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedConv1d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1,\n",
    "                 padding=0, dilation=1, groups=1, bias=True):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv1d(in_channels, out_channels, kernel_size, stride=stride,\n",
    "                              padding=padding, dilation=dilation, groups=groups, bias=bias)\n",
    "        self.padding = padding\n",
    "        self.dilation = dilation\n",
    "        self.stride = stride\n",
    "        self.kernel_size = kernel_size\n",
    "\n",
    "    def forward(self, x, length):\n",
    "        max_length = x.size(2)\n",
    "        length = torch.div(((length + 2 * self.padding - self.dilation * (self.kernel_size - 1) - 1).float() + self.stride),\n",
    "                           self.stride, rounding_mode='floor').long()\n",
    "        mask = torch.arange(max_length, device=x.device)[None, :] < length[:, None]\n",
    "        x = x * mask.unsqueeze(1)\n",
    "        x = self.conv(x)\n",
    "        return x, length\n",
    "\n",
    "class JasperBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, dropout=0.0, residual=False):\n",
    "        super().__init__()\n",
    "        self.mconv = nn.ModuleList([\n",
    "            MaskedConv1d(in_channels, in_channels, kernel_size, stride=stride, padding=padding, dilation=dilation, groups=in_channels, bias=False),\n",
    "            MaskedConv1d(in_channels, out_channels, kernel_size=1, stride=1, bias=False),\n",
    "            nn.BatchNorm1d(out_channels)\n",
    "        ])\n",
    "        self.res = None\n",
    "        if residual:\n",
    "            self.res = nn.ModuleList([\n",
    "                nn.ModuleList([\n",
    "                    MaskedConv1d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
    "                    nn.BatchNorm1d(out_channels)\n",
    "                ])\n",
    "            ])\n",
    "        self.mout = nn.Sequential(\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, length):\n",
    "        residual = x\n",
    "        res_length = length\n",
    "        out = x\n",
    "        out_length = length\n",
    "        for layer in self.mconv:\n",
    "            if isinstance(layer, MaskedConv1d):\n",
    "                out, out_length = layer(out, out_length)\n",
    "            else:\n",
    "                out = layer(out)\n",
    "        if self.res:\n",
    "            for res_layer_list in self.res:\n",
    "                res = residual\n",
    "                for layer in res_layer_list:\n",
    "                    if isinstance(layer, MaskedConv1d):\n",
    "                        res, _ = layer(res, res_length)\n",
    "                    else:\n",
    "                        res = layer(res)\n",
    "                residual = res\n",
    "        if self.res is not None:\n",
    "            out = out + residual\n",
    "        out = self.mout(out)\n",
    "        return out, out_length\n",
    "\n",
    "class ConvASREncoder(nn.Module):\n",
    "    def __init__(self, in_channels, blocks_params):\n",
    "        super().__init__()\n",
    "        layers = [JasperBlock(**params) for params in blocks_params]\n",
    "        self.encoder = nn.Sequential(*layers)\n",
    "        self.in_channels = in_channels\n",
    "\n",
    "    def forward(self, x, length):\n",
    "        for layer in self.encoder:\n",
    "            x, length = layer(x, length)\n",
    "        return x, length\n",
    "\n",
    "class AudioToMFCCPreprocessor(nn.Module):\n",
    "    def __init__(self, sample_rate=16000, n_mels=64, n_mfcc=64, n_fft=512, hop_length=160, f_min=0, f_max=8000):\n",
    "        super().__init__()\n",
    "        self.featurizer = T.MFCC(\n",
    "            sample_rate=sample_rate,\n",
    "            n_mfcc=n_mfcc,\n",
    "            melkwargs={\n",
    "                \"n_fft\": n_fft,\n",
    "                \"n_mels\": n_mels,\n",
    "                \"hop_length\": hop_length,\n",
    "                \"f_min\": f_min,\n",
    "                \"f_max\": f_max,\n",
    "            },\n",
    "        )\n",
    "\n",
    "    def forward(self, x, length):\n",
    "        with torch.no_grad():\n",
    "            x = self.featurizer(x)\n",
    "        return x, length\n",
    "\n",
    "class ConvASRDecoderClassification(nn.Module):\n",
    "    def __init__(self, in_features, num_classes):\n",
    "        super().__init__()\n",
    "        self.pooling = nn.AdaptiveAvgPool1d(output_size=1)\n",
    "        self.decoder_layers = nn.Sequential(\n",
    "            nn.Linear(in_features, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pooling(x)\n",
    "        x = x.squeeze(2)\n",
    "        x = self.decoder_layers(x)\n",
    "        return x\n",
    "\n",
    "class TopKClassificationAccuracy(nn.Module):\n",
    "    def __init__(self, k=(1,)):\n",
    "        super().__init__()\n",
    "        self.k = k\n",
    "\n",
    "    def forward(self, logits, targets):\n",
    "        with torch.no_grad():\n",
    "            maxk = max(self.k)\n",
    "            batch_size = targets.size(0)\n",
    "            _, pred = logits.topk(maxk, 1, True, True)\n",
    "            pred = pred.t()\n",
    "            correct = pred.eq(targets.view(1, -1).expand_as(pred))\n",
    "            res = []\n",
    "            for k in self.k:\n",
    "                correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n",
    "                res.append(correct_k.mul_(100.0 / batch_size))\n",
    "            return res[0] if len(res) == 1 else res\n",
    "\n",
    "class EncDecClassificationModel(nn.Module):\n",
    "    def __init__(self, num_classes, sample_rate=16000, n_mels=64, n_mfcc=64, n_fft=512, hop_length=160, f_min=0, f_max=8000):\n",
    "        super().__init__()\n",
    "        self.preprocessor = AudioToMFCCPreprocessor(\n",
    "            sample_rate=sample_rate, n_mels=n_mels, n_mfcc=n_mfcc,\n",
    "            n_fft=n_fft, hop_length=hop_length, f_min=f_min, f_max=f_max)\n",
    "        blocks_params = [\n",
    "            {\"in_channels\": n_mfcc, \"out_channels\": 128, \"kernel_size\": 11, \"stride\": 1, \"padding\": 5, \"dilation\": 1, \"dropout\": 0.0, \"residual\": False},\n",
    "            {\"in_channels\": 128, \"out_channels\": 64, \"kernel_size\": 13, \"stride\": 1, \"padding\": 6, \"dilation\": 1, \"dropout\": 0.0, \"residual\": True},\n",
    "            {\"in_channels\": 64, \"out_channels\": 64, \"kernel_size\": 15, \"stride\": 1, \"padding\": 7, \"dilation\": 1, \"dropout\": 0.0, \"residual\": True},\n",
    "            {\"in_channels\": 64, \"out_channels\": 64, \"kernel_size\": 17, \"stride\": 1, \"padding\": 8, \"dilation\": 1, \"dropout\": 0.0, \"residual\": True},\n",
    "            {\"in_channels\": 64, \"out_channels\": 128, \"kernel_size\": 29, \"stride\": 1, \"padding\": 28, \"dilation\": 2, \"dropout\": 0.0, \"residual\": False},\n",
    "            {\"in_channels\": 128, \"out_channels\": 128, \"kernel_size\": 1, \"stride\": 1, \"padding\": 0, \"dilation\": 1, \"dropout\": 0.0, \"residual\": False},\n",
    "        ]\n",
    "        self.encoder = ConvASREncoder(in_channels=n_mfcc, blocks_params=blocks_params)\n",
    "        self.decoder = ConvASRDecoderClassification(in_features=128, num_classes=num_classes)\n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "        self._accuracy = TopKClassificationAccuracy()\n",
    "\n",
    "    def forward(self, x, length, y=None):\n",
    "        x, length = self.preprocessor(x, length)\n",
    "        x, length = self.encoder(x, length)\n",
    "        logits = self.decoder(x)\n",
    "        if y is not None:\n",
    "            loss = self.loss(logits, y)\n",
    "            acc = self._accuracy(logits, y)\n",
    "            return loss, acc, logits\n",
    "        else:\n",
    "            return logits\n",
    "\n",
    "    def predict(self, x, length):\n",
    "        with torch.no_grad():\n",
    "            logits = self.forward(x, length)\n",
    "            return torch.argmax(logits, dim=-1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# <center>Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, train_loader, optimizer, device, epoch):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    running_accuracy = 0.0\n",
    "    total_samples = 0\n",
    "\n",
    "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch} [Train]\", leave=False)\n",
    "    for waveforms, lengths, labels in progress_bar:\n",
    "        waveforms = waveforms.to(device)\n",
    "        lengths = torch.tensor([lengths]).to(device) if isinstance(lengths, int) else lengths.to(device)\n",
    "        labels = torch.tensor(labels).to(device) if isinstance(labels, int) else labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss, acc, _ = model(waveforms, lengths, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        batch_size = waveforms.size(0)\n",
    "        running_loss += loss.item() * batch_size\n",
    "        running_accuracy += acc.item() * batch_size\n",
    "        total_samples += batch_size\n",
    "\n",
    "        progress_bar.set_postfix({\"loss\": f\"{loss.item():.4f}\", \"acc\": f\"{acc.item():.2f}%\"})\n",
    "    epoch_loss = running_loss / total_samples\n",
    "    epoch_accuracy = running_accuracy / total_samples\n",
    "    return epoch_loss, epoch_accuracy\n",
    "\n",
    "def validate_epoch(model, val_loader, device, epoch):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    running_accuracy = 0.0\n",
    "    total_samples = 0\n",
    "\n",
    "    progress_bar = tqdm(val_loader, desc=f\"Epoch {epoch} [Val]\", leave=False)\n",
    "    with torch.no_grad():\n",
    "        for waveforms, lengths, labels in progress_bar:\n",
    "            waveforms = waveforms.to(device)\n",
    "            lengths = torch.tensor([lengths]).to(device) if isinstance(lengths, int) else lengths.to(device)\n",
    "            labels = torch.tensor(labels).to(device) if isinstance(labels, int) else labels.to(device)\n",
    "\n",
    "            loss, acc, _ = model(waveforms, lengths, labels)\n",
    "            batch_size = waveforms.size(0)\n",
    "            running_loss += loss.item() * batch_size\n",
    "            running_accuracy += acc.item() * batch_size\n",
    "            total_samples += batch_size\n",
    "\n",
    "            progress_bar.set_postfix({\"loss\": f\"{loss.item():.4f}\", \"acc\": f\"{acc.item():.2f}%\"})\n",
    "    epoch_loss = running_loss / total_samples\n",
    "    epoch_accuracy = running_accuracy / total_samples\n",
    "    return epoch_loss, epoch_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing datasets...\n",
      "Found 35 labels: ['backward', 'bed', 'bird', 'cat', 'dog', 'down', 'eight', 'five', 'follow', 'forward', 'four', 'go', 'happy', 'house', 'learn', 'left', 'marvin', 'nine', 'no', 'off', 'on', 'one', 'right', 'seven', 'sheila', 'six', 'stop', 'three', 'tree', 'two', 'up', 'visual', 'wow', 'yes', 'zero']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading training data:  15%|█▍        | 15473/105829 [00:24<02:25, 620.14it/s]"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "num_epochs = 5\n",
    "batch_size = 64\n",
    "learning_rate = 0.001\n",
    "sample_rate = 16000\n",
    "fixed_length = sample_rate  # 1 second clips (SpeechCommands are ~1 sec)\n",
    "\n",
    "# Prepare datasets (in-memory)\n",
    "print(\"Preparing datasets...\")\n",
    "train_dataset = InMemorySpeechCommands(subset=\"training\", fixed_length=fixed_length)\n",
    "val_dataset = InMemorySpeechCommands(subset=\"validation\", fixed_length=fixed_length)\n",
    "num_classes = len(train_dataset.label_set)\n",
    "print(\"Label set:\", train_dataset.label_set)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, pin_memory=True)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# Initialize model, optimizer\n",
    "model = EncDecClassificationModel(num_classes=num_classes, sample_rate=sample_rate).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Wrap epoch loop with tqdm for overall progress monitoring\n",
    "for epoch in tqdm(range(1, num_epochs + 1), desc=\"Epochs\"):\n",
    "    train_loss, train_acc = train_epoch(model, train_loader, optimizer, device, epoch)\n",
    "    val_loss, val_acc = validate_epoch(model, val_loader, device, epoch)\n",
    "    print(f\"Epoch {epoch}: Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%, \"\n",
    "          f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
    "    # Save the best model\n",
    "    if val_acc > 0:\n",
    "        torch.save(model.state_dict(), \"best_model.pth\")\n",
    "        print(\"Saved best model!\")\n",
    "print(\"Training complete.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
